{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to train the custom tflite model against mobile phone image and get it verified with test image.  \n",
    "The model is used to run in the raspberry pi car to locate the lost phone on the ground.  \n",
    "Thus, the model is a tflite model which is tuned for IOT device. And the train images are all phone images on the ground.  \n",
    "\n",
    "# Reference\n",
    "https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector  \n",
    "https://www.tensorflow.org/lite/tutorials/model_maker_object_detection  \n",
    "https://www.tensorflow.org/lite/guide/model_maker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tflite-model-maker\n",
    "!pip3 install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tflite_model_maker import object_detector\n",
    "from tflite_model_maker import model_spec\n",
    "\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unitilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image, exclude imcompatible image, remove alpha channel\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.mkdir(output_dir)    \n",
    "    count = 0\n",
    "    for f in os.listdir(input_dir):\n",
    "      if f.endswith('jpeg') or f.endswith('jpg'):\n",
    "        print('pre process file: ', f)\n",
    "        image = Image.open(os.path.join(input_dir, f), 'r')\n",
    "        if image.mode == 'RGBA':\n",
    "          image = image.convert('RGB')\n",
    "        image.save(os.path.join(output_dir, f))\n",
    "        count += 1\n",
    "    print('processed ', count, ' images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input Files for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.path.abspath('./mobile_phone')\n",
    "train_image_dir = os.path.join(BASE_PATH, 'train')\n",
    "train_process_image_dir = os.path.join(BASE_PATH, 'train_preprocess')\n",
    "train_label_dir = os.path.join(BASE_PATH,'train_label')\n",
    "\n",
    "test_image_dir = os.path.join(BASE_PATH,'test')\n",
    "test_process_image_dir = os.path.join(BASE_PATH,'test_preprocess')\n",
    "test_label_dir = os.path.join(BASE_PATH,'test_label')\n",
    "\n",
    "label_map = {1: 'phone'}\n",
    "\n",
    "# preprocess_images(train_image_dir, train_process_image_dir)\n",
    "# preprocess_images(test_image_dir, test_process_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUAL ACTION: Use LabelImg to label the input image, the label path is   \n",
    "\n",
    "$BASE_PATH/train_label  \n",
    "$BASE_PATH/test_label\n",
    "\n",
    "LabelImg Usage: https://github.com/tzutalin/labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_process_image_dir = os.path.join(BASE_PATH, 'train_preprocess')\n",
    "test_label_dir = os.path.join(BASE_PATH,'test_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = object_detector.DataLoader.from_pascal_voc(train_process_image_dir, train_label_dir, label_map=label_map)\n",
    "spec = model_spec.get('efficientdet_lite1')\n",
    "\n",
    "test_data_loader = object_detector.DataLoader.from_pascal_voc(test_process_image_dir, test_label_dir, label_map=label_map)\n",
    "model = object_detector.create(train_data_loader, spec, validation_data=test_data_loader, batch_size=5, epochs=50, train_whole_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = object_detector.DataLoader.from_pascal_voc(test_process_image_dir, test_label_dir, label_map=label_map)\n",
    "model.evaluate(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(BASE_PATH, 'model')\n",
    "model_name = 'phone-1.tflite'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "model.export(model_dir, tflite_filename=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate TFLite Model with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_tflite(model_path, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify with Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility\n",
    "import cv2\n",
    "\n",
    "# Load the labels into a list\n",
    "# classes = ['???'] * model.model_spec.config.num_classes\n",
    "# label_map = model.model_spec.config.label_map\n",
    "# for label_id, label_name in label_map.as_dict().items():\n",
    "#   classes[label_id-1] = label_name\n",
    "\n",
    "classes = {0: 'phone'}\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "  signature_fn = interpreter.get_signature_runner()\n",
    "\n",
    "  # Feed the input image to the model\n",
    "  output = signature_fn(images=image)\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  count = int(np.squeeze(output['output_0']))\n",
    "  scores = np.squeeze(output['output_1'])\n",
    "  classes = np.squeeze(output['output_2'])\n",
    "  boxes = np.squeeze(output['output_3'])\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path,\n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute\n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_THRESHOLD = 0.6\n",
    "model_dir = os.path.join(BASE_PATH, 'model')\n",
    "model_name = 'phone-1.tflite'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "test_image_dir = model_dir = os.path.join(BASE_PATH, 'test_real_image')\n",
    "\n",
    "test_image = os.path.join(test_image_dir, 'test1.jpeg')\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "##### Image 1\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    test_image,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Image 2\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "test_image2 = os.path.join(test_image_dir, 'test2.jpeg')\n",
    "\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    test_image2,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Image 3\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "test_image3 = os.path.join(test_image_dir, 'test3.jpeg')\n",
    "\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    test_image3,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
