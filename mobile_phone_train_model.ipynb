{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to train the custom tflite model against mobile phone image and get it verified with test image.  \n",
    "The model is used to run in the raspberry pi car to locate the lost phone on the ground.  \n",
    "Thus, the model is a tflite model which is tuned for IOT device. And the train images are all phone images on the ground.  \n",
    "\n",
    "# Reference\n",
    "https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector  \n",
    "https://www.tensorflow.org/lite/tutorials/model_maker_object_detection  \n",
    "https://www.tensorflow.org/lite/guide/model_maker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tflite-model-maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tflite_model_maker import object_detector\n",
    "from tflite_model_maker import model_spec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unitilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image, exclude imcompatible image, remove alpha channel\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.mkdir(output_dir)    \n",
    "    count = 0\n",
    "    for f in os.listdir(input_dir):\n",
    "      if f.endswith('jpeg') or f.endswith('jpg'):\n",
    "        print('pre process file: ', f)\n",
    "        image = Image.open(os.path.join(input_dir, f), 'r')\n",
    "        if image.mode == 'RGBA':\n",
    "          image = image.convert('RGB')\n",
    "        image.save(os.path.join(output_dir, f))\n",
    "        count += 1\n",
    "    print('processed ', count, ' images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input Files for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.path.abspath('./mobile_phone')\n",
    "train_image_dir = os.path.join(BASE_PATH, 'train')\n",
    "train_process_image_dir = os.path.join(BASE_PATH, 'train_preprocess')\n",
    "train_label_dir = os.path.join(BASE_PATH,'train_label')\n",
    "\n",
    "test_image_dir = os.path.join(BASE_PATH,'test')\n",
    "test_process_image_dir = os.path.join(BASE_PATH,'test_preprocess')\n",
    "test_label_dir = os.path.join(BASE_PATH,'test_label')\n",
    "\n",
    "label_map = {1: 'phone'}\n",
    "\n",
    "# preprocess_images(train_image_dir, train_process_image_dir)\n",
    "# preprocess_images(test_image_dir, test_process_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUAL ACTION: Use LabelImg to label the input image, the label path is   \n",
    "\n",
    "$BASE_PATH/train_label  \n",
    "$BASE_PATH/test_label\n",
    "\n",
    "LabelImg Usage: https://github.com/tzutalin/labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_process_image_dir = os.path.join(BASE_PATH, 'train_preprocess')\n",
    "test_label_dir = os.path.join(BASE_PATH,'test_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Cache will be stored in /var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpfubars_c with prefix filename 06dc7edd3f1e6e74d1358b2e7b35f5fc. Cache_prefix is /var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpfubars_c/06dc7edd3f1e6e74d1358b2e7b35f5fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Cache will be stored in /var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpfubars_c with prefix filename 06dc7edd3f1e6e74d1358b2e7b35f5fc. Cache_prefix is /var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpfubars_c/06dc7edd3f1e6e74d1358b2e7b35f5fc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:On image 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:On image 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 22:34:55.971507: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 100s 20s/step - det_loss: 1.7815 - cls_loss: 1.1634 - box_loss: 0.0124 - reg_l2_loss: 0.1078 - loss: 1.8893 - learning_rate: 0.0078 - gradient_norm: 1.4035\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 54s 18s/step - det_loss: 1.7519 - cls_loss: 1.1516 - box_loss: 0.0120 - reg_l2_loss: 0.1078 - loss: 1.8597 - learning_rate: 0.0075 - gradient_norm: 1.2494\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 1.7127 - cls_loss: 1.1247 - box_loss: 0.0118 - reg_l2_loss: 0.1078 - loss: 1.8205 - learning_rate: 0.0075 - gradient_norm: 1.3569\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 1.6554 - cls_loss: 1.0980 - box_loss: 0.0111 - reg_l2_loss: 0.1078 - loss: 1.7632 - learning_rate: 0.0074 - gradient_norm: 1.5381\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 59s 21s/step - det_loss: 1.5733 - cls_loss: 1.0447 - box_loss: 0.0106 - reg_l2_loss: 0.1078 - loss: 1.6811 - learning_rate: 0.0074 - gradient_norm: 1.6595\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 58s 20s/step - det_loss: 1.4090 - cls_loss: 0.9582 - box_loss: 0.0090 - reg_l2_loss: 0.1078 - loss: 1.5168 - learning_rate: 0.0073 - gradient_norm: 2.9700\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 1.3188 - cls_loss: 0.8635 - box_loss: 0.0091 - reg_l2_loss: 0.1078 - loss: 1.4266 - learning_rate: 0.0072 - gradient_norm: 3.8030\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 1.0639 - cls_loss: 0.6753 - box_loss: 0.0078 - reg_l2_loss: 0.1078 - loss: 1.1717 - learning_rate: 0.0071 - gradient_norm: 2.2842\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 54s 19s/step - det_loss: 1.0373 - cls_loss: 0.5995 - box_loss: 0.0088 - reg_l2_loss: 0.1078 - loss: 1.1451 - learning_rate: 0.0070 - gradient_norm: 3.7221\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 0.9022 - cls_loss: 0.5357 - box_loss: 0.0073 - reg_l2_loss: 0.1078 - loss: 1.0100 - learning_rate: 0.0068 - gradient_norm: 4.4816\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 56s 20s/step - det_loss: 0.6835 - cls_loss: 0.4000 - box_loss: 0.0057 - reg_l2_loss: 0.1078 - loss: 0.7913 - learning_rate: 0.0067 - gradient_norm: 2.0506\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 54s 19s/step - det_loss: 0.7133 - cls_loss: 0.4181 - box_loss: 0.0059 - reg_l2_loss: 0.1078 - loss: 0.8211 - learning_rate: 0.0065 - gradient_norm: 2.7180\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 54s 19s/step - det_loss: 0.6438 - cls_loss: 0.3892 - box_loss: 0.0051 - reg_l2_loss: 0.1078 - loss: 0.7516 - learning_rate: 0.0064 - gradient_norm: 2.2686\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.5452 - cls_loss: 0.3311 - box_loss: 0.0043 - reg_l2_loss: 0.1078 - loss: 0.6530 - learning_rate: 0.0062 - gradient_norm: 1.9155\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 52s 18s/step - det_loss: 0.4971 - cls_loss: 0.3132 - box_loss: 0.0037 - reg_l2_loss: 0.1078 - loss: 0.6049 - learning_rate: 0.0060 - gradient_norm: 1.7284\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.5113 - cls_loss: 0.3263 - box_loss: 0.0037 - reg_l2_loss: 0.1078 - loss: 0.6191 - learning_rate: 0.0058 - gradient_norm: 2.2066\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 54s 19s/step - det_loss: 0.4715 - cls_loss: 0.2853 - box_loss: 0.0037 - reg_l2_loss: 0.1078 - loss: 0.5793 - learning_rate: 0.0056 - gradient_norm: 1.9460\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 56s 20s/step - det_loss: 0.4623 - cls_loss: 0.2889 - box_loss: 0.0035 - reg_l2_loss: 0.1078 - loss: 0.5701 - learning_rate: 0.0054 - gradient_norm: 3.3816\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 0.4793 - cls_loss: 0.3183 - box_loss: 0.0032 - reg_l2_loss: 0.1078 - loss: 0.5871 - learning_rate: 0.0052 - gradient_norm: 2.1525\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.4515 - cls_loss: 0.2875 - box_loss: 0.0033 - reg_l2_loss: 0.1078 - loss: 0.5593 - learning_rate: 0.0050 - gradient_norm: 2.3490\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 53s 19s/step - det_loss: 0.3640 - cls_loss: 0.2445 - box_loss: 0.0024 - reg_l2_loss: 0.1078 - loss: 0.4718 - learning_rate: 0.0047 - gradient_norm: 1.4570\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.3950 - cls_loss: 0.2785 - box_loss: 0.0023 - reg_l2_loss: 0.1078 - loss: 0.5028 - learning_rate: 0.0045 - gradient_norm: 1.9849\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 55s 19s/step - det_loss: 0.3938 - cls_loss: 0.2677 - box_loss: 0.0025 - reg_l2_loss: 0.1078 - loss: 0.5017 - learning_rate: 0.0042 - gradient_norm: 2.1614\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.3411 - cls_loss: 0.2450 - box_loss: 0.0019 - reg_l2_loss: 0.1078 - loss: 0.4489 - learning_rate: 0.0040 - gradient_norm: 1.7494\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.3976 - cls_loss: 0.2509 - box_loss: 0.0029 - reg_l2_loss: 0.1078 - loss: 0.5054 - learning_rate: 0.0038 - gradient_norm: 1.7521\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.3342 - cls_loss: 0.2376 - box_loss: 0.0019 - reg_l2_loss: 0.1078 - loss: 0.4420 - learning_rate: 0.0035 - gradient_norm: 1.7824\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 54s 18s/step - det_loss: 0.3510 - cls_loss: 0.2361 - box_loss: 0.0023 - reg_l2_loss: 0.1078 - loss: 0.4588 - learning_rate: 0.0033 - gradient_norm: 2.2797\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 53s 18s/step - det_loss: 0.2907 - cls_loss: 0.2108 - box_loss: 0.0016 - reg_l2_loss: 0.1078 - loss: 0.3985 - learning_rate: 0.0031 - gradient_norm: 1.3147\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 54s 19s/step - det_loss: 0.3169 - cls_loss: 0.2330 - box_loss: 0.0017 - reg_l2_loss: 0.1078 - loss: 0.4247 - learning_rate: 0.0028 - gradient_norm: 2.0141\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - ETA: 0s - det_loss: 0.3666 - cls_loss: 0.2531 - box_loss: 0.0023 - reg_l2_loss: 0.1078 - loss: 0.4744 - learning_rate: 0.0026 - gradient_norm: 1.9623 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 23:02:50.368034: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:138 : RESOURCE_EXHAUSTED: /var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpl97081jo/ckpt-30_temp/part-00000-of-00001.data-00000-of-00001.tempstate2227047178749386079; No space left on device\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "/var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpl97081jo/ckpt-30_temp/part-00000-of-00001.data-00000-of-00001.tempstate2227047178749386079; No space left on device [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/Users/twinssbc/VSProject/travel-robot/mobile_phone_train_model.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/twinssbc/VSProject/travel-robot/mobile_phone_train_model.ipynb#ch0000013?line=0'>1</a>\u001b[0m train_data_loader \u001b[39m=\u001b[39m object_detector\u001b[39m.\u001b[39mDataLoader\u001b[39m.\u001b[39mfrom_pascal_voc(train_process_image_dir, train_label_dir, label_map\u001b[39m=\u001b[39mlabel_map)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/twinssbc/VSProject/travel-robot/mobile_phone_train_model.ipynb#ch0000013?line=1'>2</a>\u001b[0m spec \u001b[39m=\u001b[39m model_spec\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mefficientdet_lite4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/twinssbc/VSProject/travel-robot/mobile_phone_train_model.ipynb#ch0000013?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m object_detector\u001b[39m.\u001b[39;49mcreate(train_data_loader, spec, batch_size\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, train_whole_model\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:260\u001b[0m, in \u001b[0;36mObjectDetector.create\u001b[0;34m(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=257'>258</a>\u001b[0m \u001b[39mif\u001b[39;00m do_train:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=258'>259</a>\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRetraining the models...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=259'>260</a>\u001b[0m   object_detector\u001b[39m.\u001b[39;49mtrain(train_data, validation_data, epochs, batch_size)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=260'>261</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=261'>262</a>\u001b[0m   object_detector\u001b[39m.\u001b[39mcreate_model()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:123\u001b[0m, in \u001b[0;36mObjectDetector.train\u001b[0;34m(self, train_data, validation_data, epochs, batch_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=118'>119</a>\u001b[0m train_ds, steps_per_epoch, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=119'>120</a>\u001b[0m     train_data, batch_size, is_training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=120'>121</a>\u001b[0m validation_ds, validation_steps, val_json_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=121'>122</a>\u001b[0m     validation_data, batch_size, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=122'>123</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_spec\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, train_ds, steps_per_epoch,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=123'>124</a>\u001b[0m                              validation_ds, validation_steps, epochs,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py?line=124'>125</a>\u001b[0m                              batch_size, val_json_file)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:266\u001b[0m, in \u001b[0;36mEfficientDetModelSpec.train\u001b[0;34m(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=263'>264</a>\u001b[0m train\u001b[39m.\u001b[39msetup_model(model, config)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=264'>265</a>\u001b[0m train\u001b[39m.\u001b[39minit_experimental(config)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=265'>266</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=266'>267</a>\u001b[0m     train_dataset,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=267'>268</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=268'>269</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=269'>270</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mtrain_lib\u001b[39m.\u001b[39;49mget_callbacks(config\u001b[39m.\u001b[39;49mas_dict(), val_dataset),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=270'>271</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=271'>272</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py?line=272'>273</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: /var/folders/t6/xm8bbqvd10jcg3mxv5fdfq640000gn/T/tmpl97081jo/ckpt-30_temp/part-00000-of-00001.data-00000-of-00001.tempstate2227047178749386079; No space left on device [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "train_data_loader = object_detector.DataLoader.from_pascal_voc(train_process_image_dir, train_label_dir, label_map=label_map)\n",
    "spec = model_spec.get('efficientdet_lite4')\n",
    "\n",
    "test_data_loader = object_detector.DataLoader.from_pascal_voc(test_process_image_dir, test_label_dir, label_map=label_map)\n",
    "model = object_detector.create(train_data_loader, spec, validation_data=test_data_loader, batch_size=5, epochs=50, train_whole_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = object_detector.DataLoader.from_pascal_voc(test_process_image_dir, test_label_dir, label_map=label_map)\n",
    "model.evaluate(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(BASE_PATH, 'model')\n",
    "model_path = os.path.join(model_dir, 'model.tflite')\n",
    "model.export(model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate TFLite Model with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_tflite(model_path, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify with Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility\n",
    "# Load the labels into a list\n",
    "classes = ['???'] * model.model_spec.config.num_classes\n",
    "label_map = model.model_spec.config.label_map\n",
    "for label_id, label_name in label_map.as_dict().items():\n",
    "  classes[label_id-1] = label_name\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "  signature_fn = interpreter.get_signature_runner()\n",
    "\n",
    "  # Feed the input image to the model\n",
    "  output = signature_fn(images=image)\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  count = int(np.squeeze(output['output_0']))\n",
    "  scores = np.squeeze(output['output_1'])\n",
    "  classes = np.squeeze(output['output_2'])\n",
    "  boxes = np.squeeze(output['output_3'])\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path,\n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute\n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_THRESHOLD = 0.6\n",
    "\n",
    "test_image_dir = model_dir = os.path.join(BASE_PATH, 'test_real_image')\n",
    "\n",
    "test_image = os.path.join(test_image_dir, 'test1.jpeg')\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "##### Image 1\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    test_image,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Image 2\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "test_image2 = os.path.join(test_image_dir, 'test2.jpeg')\n",
    "\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    test_image2,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Image 3\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "test_image3 = os.path.join(test_image_dir, 'test3.jpeg')\n",
    "\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    test_image3,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
