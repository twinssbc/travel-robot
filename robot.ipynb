{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os,time\n",
    "import ipywidgets.widgets as widgets\n",
    "from object_detector import ObjectDetector\n",
    "from object_detector import ObjectDetectorOptions\n",
    "import utils\n",
    "\n",
    "# try:\n",
    "#   # Import TFLite interpreter from tflite_runtime package if it's available.\n",
    "#   from tflite_runtime.interpreter import Interpreter\n",
    "#   from tflite_runtime.interpreter import load_delegate\n",
    "# except ImportError:\n",
    "#   # If not, fallback to use the TFLite interpreter from the full TF package.\n",
    "#   import tensorflow as tf\n",
    "\n",
    "#   Interpreter = tf.lite.Interpreter\n",
    "#   load_delegate = tf.lite.experimental.load_delegate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init camera \n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320) # set Width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240) # set Height\n",
    "cap.set(cv2.CAP_PROP_FRAME_COUNT, 30)  #设置帧率\n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))\n",
    "# cap.set(cv2.CAP_PROP_BRIGHTNESS, 40) #设置亮度 -64 - 64  0.0\n",
    "# cap.set(cv2.CAP_PROP_CONTRAST, 50) #设置对比度 -64 - 64  2.0\n",
    "# cap.set(cv2.CAP_PROP_EXPOSURE, 156)  #设置曝光值 1.0 - 5000  156.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3e112373f94285a6370187b5e71d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpg', height='240', width='320')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_widget = widgets.Image(format='jpg', width=320, height=240)\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization parameters\n",
    "row_size = 20  # pixels\n",
    "left_margin = 24  # pixels\n",
    "text_color = (0, 0, 255)  # red\n",
    "font_size = 1\n",
    "font_thickness = 1\n",
    "fps_avg_frame_count = 10\n",
    "model = './mobile_phone/model/phone-1.tflite'\n",
    "num_threads = 4\n",
    "enable_edgetpu = False\n",
    "\n",
    "# Initialize the object detection model\n",
    "options = ObjectDetectorOptions(\n",
    "    num_threads=num_threads,\n",
    "    score_threshold=0.5,\n",
    "    max_results=3,\n",
    "    enable_edgetpu=enable_edgetpu)\n",
    "detector = ObjectDetector(model_path=model, options=options)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation: 1 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 1 end detecting\n",
      "interation: 2 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 2 end detecting\n",
      "interation: 3 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 3 end detecting\n",
      "interation: 4 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 4 end detecting\n",
      "interation: 5 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 5 end detecting\n",
      "interation: 6 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 6 end detecting\n",
      "interation: 7 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 7 end detecting\n",
      "interation: 8 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 8 end detecting\n",
      "interation: 9 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 9 end detecting\n",
      "interation: 10 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 10 end detecting\n",
      "interation: 11 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 11 end detecting\n",
      "interation: 12 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 12 end detecting\n",
      "interation: 13 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 13 end detecting\n",
      "interation: 14 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 14 end detecting\n",
      "interation: 15 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 15 end detecting\n",
      "interation: 16 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 16 end detecting\n",
      "interation: 17 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 17 end detecting\n",
      "interation: 18 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 18 end detecting\n",
      "interation: 19 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 19 end detecting\n",
      "interation: 20 start detecting\n",
      "before invoke\n",
      "after invoke\n",
      "interation: 20 end detecting\n",
      "interation: 21 start detecting\n",
      "before invoke\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\travel-robot\\robot.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/travel-robot/robot.ipynb#ch0000005?line=14'>15</a>\u001b[0m \u001b[39m# Run object detection estimation using the model.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/travel-robot/robot.ipynb#ch0000005?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minteration:\u001b[39m\u001b[39m\"\u001b[39m, counter, \u001b[39m\"\u001b[39m\u001b[39mstart detecting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/travel-robot/robot.ipynb#ch0000005?line=16'>17</a>\u001b[0m detections \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mdetect(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/travel-robot/robot.ipynb#ch0000005?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minteration:\u001b[39m\u001b[39m\"\u001b[39m, counter, \u001b[39m\"\u001b[39m\u001b[39mend detecting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/travel-robot/robot.ipynb#ch0000005?line=19'>20</a>\u001b[0m \u001b[39m# Draw keypoints and edges on input image\u001b[39;00m\n",
      "File \u001b[1;32md:\\Project\\travel-robot\\object_detector.py:195\u001b[0m, in \u001b[0;36mObjectDetector.detect\u001b[1;34m(self, input_image)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Project/travel-robot/object_detector.py?line=191'>192</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_input_tensor(input_tensor)\n\u001b[0;32m    <a href='file:///d%3A/Project/travel-robot/object_detector.py?line=193'>194</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbefore invoke\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='file:///d%3A/Project/travel-robot/object_detector.py?line=194'>195</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49minvoke()\n\u001b[0;32m    <a href='file:///d%3A/Project/travel-robot/object_detector.py?line=195'>196</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter invoke\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/Project/travel-robot/object_detector.py?line=197'>198</a>\u001b[0m \u001b[39m# Get all output details\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:916\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=903'>904</a>\u001b[0m \u001b[39m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=904'>905</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=905'>906</a>\u001b[0m \u001b[39mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=912'>913</a>\u001b[0m \u001b[39m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=913'>914</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=914'>915</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> <a href='file:///c%3A/Users/twinssbc/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/lite/python/interpreter.py?line=915'>916</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mInvoke()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continuously capture images from the camera and run inference\n",
    "counter, fps = 0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "  success, image = cap.read()\n",
    "  if not success:\n",
    "    sys.exit(\n",
    "        'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "    )\n",
    "\n",
    "  counter += 1\n",
    "  image = cv2.flip(image, 1)\n",
    "\n",
    "  # Run object detection estimation using the model.\n",
    "  print(\"interation:\", counter, \"start detecting\")\n",
    "  detections = detector.detect(image)\n",
    "  print(\"interation:\", counter, \"end detecting\")\n",
    "\n",
    "  # Draw keypoints and edges on input image\n",
    "  image = utils.visualize(image, detections)\n",
    "\n",
    "  # Calculate the FPS\n",
    "  if counter % fps_avg_frame_count == 0:\n",
    "    end_time = time.time()\n",
    "    fps = fps_avg_frame_count / (end_time - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "  # Show the FPS\n",
    "  fps_text = 'FPS = {:.1f}'.format(fps)\n",
    "  text_location = (left_margin, row_size)\n",
    "  cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "              font_size, text_color, font_thickness)\n",
    "  image_widget.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "  # Stop the program if the ESC key is pressed.\n",
    "  # if keyboard.read_key(1000) == 27:\n",
    "  #   break\n",
    "  # # cv2.imshow('object_detector', image)\n",
    "  # cap.release()\n",
    "  # cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b6ab31ecee4e0c283c19ce85e96154ecb765bc3e83d7afa01b6060731badf1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
